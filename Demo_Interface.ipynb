{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8tkIDvZsXMchm4feGyGrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PETEROA/AutoML/blob/main/Demo_Interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create Gradio web interface\n",
        "- Load all optimized models\n",
        "- Allow image upload for inference\n",
        "- Compare different model formats in real-time\n",
        "- Show speed/accuracy tradeoffs\n",
        "- Create shareable demo link\n",
        "\n"
      ],
      "metadata": {
        "id": "qkkr8hrZn4Uu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK2agfdCnwth"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision gradio -q\n",
        "!pip install onnxruntime pillow -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaXBBmS_cAgp",
        "outputId": "00817702-a31e-4b81-9a4b-d79770fb84db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import onnxruntime as ort\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")\n",
        "\n",
        "# Setup paths\n",
        "OUTPUT_DIR = Path('/content/drive/MyDrive/AutoML')\n",
        "MODELS_DIR = OUTPUT_DIR / 'models'\n",
        "DEPLOYMENT_DIR = OUTPUT_DIR / 'deployment'\n",
        "ONNX_DIR = DEPLOYMENT_DIR / 'onnx'\n",
        "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dgYT2zoTJs",
        "outputId": "10af3bd6-3101-4ee7-eaa1-75c8ca1d31a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All imports successful\n",
            "Output directory: /content/drive/MyDrive/AutoML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Deployment Summary\n",
        "# CIFAR-10 class names\n",
        "CIFAR10_CLASSES = [\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "]\n",
        "\n",
        "# Load deployment summary\n",
        "summary_file = DEPLOYMENT_DIR / 'deployment_summary.json'\n",
        "\n",
        "if summary_file.exists():\n",
        "    with open(summary_file, 'r') as f:\n",
        "        deployment_info = json.load(f)\n",
        "    print(\"âœ“ Loaded deployment summary\")\n",
        "    print(f\"  Available models: {len(deployment_info.get('models', []))}\")\n",
        "else:\n",
        "    print(\"âš  Deployment summary not found, creating demo configuration\")\n",
        "    deployment_info = {\n",
        "        'models': [{\n",
        "            'rank': 1,\n",
        "            'architecture': 'resnet18',\n",
        "            'files': {\n",
        "                'onnx': 'deployment/onnx/rank1_resnet18.onnx'\n",
        "            }\n",
        "        }]\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn9jys8iooam",
        "outputId": "0d276e1f-c38d-4e65-b2fa-f5e29760259a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš  Deployment summary not found, creating demo configuration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loading Functions\n",
        "class ModelWrapper:\n",
        "    \"\"\"Wrapper for different model formats\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, model_type='onnx'):\n",
        "        self.model_path = model_path\n",
        "        self.model_type = model_type\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the model\"\"\"\n",
        "        if self.model_type == 'onnx':\n",
        "            if Path(self.model_path).exists():\n",
        "                self.model = ort.InferenceSession(\n",
        "                    self.model_path,\n",
        "                    providers=['CPUExecutionProvider']\n",
        "                )\n",
        "                print(f\"âœ“ Loaded ONNX model: {Path(self.model_path).name}\")\n",
        "            else:\n",
        "                print(f\"âš  Model not found: {self.model_path}\")\n",
        "\n",
        "        elif self.model_type == 'pytorch':\n",
        "            # Load PyTorch model\n",
        "            # This is a placeholder - would need actual model architecture\n",
        "            pass\n",
        "\n",
        "    def predict(self, image: np.ndarray):\n",
        "        \"\"\"Run inference\"\"\"\n",
        "        if self.model is None:\n",
        "            return None, None\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if self.model_type == 'onnx':\n",
        "            # ONNX inference\n",
        "            input_name = self.model.get_inputs()[0].name\n",
        "            outputs = self.model.run(None, {input_name: image})[0]\n",
        "\n",
        "        inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
        "\n",
        "        return outputs, inference_time\n",
        "\n",
        "# Image preprocessing\n",
        "def preprocess_image(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"Preprocess image for CIFAR-10 models\"\"\"\n",
        "    # Resize to 32x32 (CIFAR-10 size)\n",
        "    image = image.resize((32, 32))\n",
        "\n",
        "    # Convert to tensor and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.4914, 0.4822, 0.4465),\n",
        "            (0.2023, 0.1994, 0.2010)\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image)\n",
        "\n",
        "    # Add batch dimension and convert to numpy\n",
        "    numpy_array = tensor.unsqueeze(0).numpy()\n",
        "\n",
        "    return numpy_array\n",
        "\n",
        "def postprocess_outputs(outputs: np.ndarray) -> dict:\n",
        "    \"\"\"Convert model outputs to predictions\"\"\"\n",
        "    # Get probabilities\n",
        "    probabilities = np.exp(outputs) / np.sum(np.exp(outputs), axis=1, keepdims=True)\n",
        "\n",
        "    # Get top 3 predictions\n",
        "    top3_idx = np.argsort(probabilities[0])[::-1][:3]\n",
        "\n",
        "    predictions = {}\n",
        "    for idx in top3_idx:\n",
        "        class_name = CIFAR10_CLASSES[idx]\n",
        "        confidence = float(probabilities[0][idx])\n",
        "        predictions[class_name] = confidence\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"âœ“ Model wrapper and preprocessing functions ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwP63jMsozd2",
        "outputId": "97f05b8f-33a8-4087-a2ca-f6b98d66b6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model wrapper and preprocessing functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Available Models\n",
        "# Load all available ONNX models\n",
        "available_models = {}\n",
        "\n",
        "print(\"Loading available models...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_info in deployment_info.get('models', []):\n",
        "    rank = model_info['rank']\n",
        "    arch = model_info['architecture']\n",
        "\n",
        "    # Try to load ONNX model\n",
        "    onnx_file = model_info['files'].get('onnx', '')\n",
        "    onnx_path = OUTPUT_DIR / onnx_file\n",
        "\n",
        "    if onnx_path.exists():\n",
        "        model_name = f\"Rank {rank} - {arch.upper()}\"\n",
        "        available_models[model_name] = ModelWrapper(str(onnx_path), 'onnx')\n",
        "        print(f\"âœ“ {model_name}\")\n",
        "    else:\n",
        "        print(f\"âš  ONNX model not found: {onnx_path}\")\n",
        "\n",
        "if len(available_models) == 0:\n",
        "    print(\"\\nâš  No models found. Demo will run in limited mode.\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Loaded {len(available_models)} models for demo\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wEmSwPnpGty",
        "outputId": "627a2ea9-bfb0-4c66-eece-d6f54753556f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading available models...\n",
            "============================================================\n",
            "âš  ONNX model not found: /content/drive/MyDrive/AutoML/deployment/onnx/rank1_resnet18.onnx\n",
            "\n",
            "âš  No models found. Demo will run in limited mode.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import onnxruntime as ort\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")\n",
        "\n",
        "# Setup paths\n",
        "OUTPUT_DIR = Path('/content/drive/MyDrive/AutoML')\n",
        "MODELS_DIR = OUTPUT_DIR / 'models'\n",
        "DEPLOYMENT_DIR = OUTPUT_DIR / 'deployment'\n",
        "ONNX_DIR = DEPLOYMENT_DIR / 'onnx'\n",
        "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Create Gradio Interface\n",
        "def classify_image(image, model_selection):\n",
        "    \"\"\"\n",
        "    Main inference function for Gradio\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return \"Please upload an image\", \"N/A\", None\n",
        "\n",
        "    if len(available_models) == 0:\n",
        "        return \"No models available\", \"N/A\", None\n",
        "\n",
        "    try:\n",
        "        # Get selected model\n",
        "        if model_selection not in available_models:\n",
        "            model_selection = list(available_models.keys())[0]\n",
        "\n",
        "        model = available_models[model_selection]\n",
        "\n",
        "        # Preprocess image\n",
        "        processed_image = preprocess_image(image)\n",
        "\n",
        "        # Run inference\n",
        "        outputs, inference_time = model.predict(processed_image)\n",
        "\n",
        "        if outputs is None:\n",
        "            return \"Model inference failed\", \"N/A\", None\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = postprocess_outputs(outputs)\n",
        "\n",
        "        # Format results\n",
        "        result_text = \"Predictions:\\n\\n\"\n",
        "        for class_name, confidence in predictions.items():\n",
        "            result_text += f\"- {class_name.capitalize()}: {confidence*100:.1f}%\\n\"\n",
        "\n",
        "        inference_text = f\"Inference Time: {inference_time:.2f} ms\"\n",
        "\n",
        "        return result_text, inference_text, predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", \"N/A\", None\n",
        "\n",
        "print(\"âœ“ Inference function ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSzZmRu-pNh6",
        "outputId": "e0d4f94b-518b-4c31-8f8c-21c00666c420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All imports successful\n",
            "Output directory: /content/drive/MyDrive/AutoML\n",
            "âœ“ Inference function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(image):\n",
        "    \"\"\"\n",
        "    Compare all models on the same image\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return \"Please upload an image\"\n",
        "\n",
        "    if len(available_models) == 0:\n",
        "        return \"No models available for comparison\"\n",
        "\n",
        "    try:\n",
        "        # Preprocess once\n",
        "        processed_image = preprocess_image(image)\n",
        "\n",
        "        comparison_text = \"# Model Comparison\\n\\n\"\n",
        "        comparison_text += \"| Model | Top Prediction | Confidence | Inference Time |\\n\"\n",
        "        comparison_text += \"|-------|----------------|------------|----------------|\\n\"\n",
        "\n",
        "        for model_name, model in available_models.items():\n",
        "            outputs, inference_time = model.predict(processed_image)\n",
        "\n",
        "            if outputs is not None:\n",
        "                predictions = postprocess_outputs(outputs)\n",
        "                top_class = list(predictions.keys())[0]\n",
        "                top_conf = list(predictions.values())[0]\n",
        "\n",
        "                comparison_text += f\"| {model_name} | {top_class} | {top_conf*100:.1f}% | {inference_time:.2f} ms |\\n\"\n",
        "\n",
        "        return comparison_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during comparison: {str(e)}\"\n",
        "\n",
        "print(\"âœ“ Comparison function ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooTBG440ptog",
        "outputId": "73aa4cef-8afb-4c91-a425-7d73b36a110e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Comparison function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Gradio Interface\n",
        "# Create interface\n",
        "with gr.Blocks(title=\"AutoML-Distill Demo\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "     AutoML-Distill: Neural Architecture Search + Knowledge Distillation\n",
        "\n",
        "    This demo showcases an automated model compression pipeline using:\n",
        "    - Neural Architecture Search (NAS)to find optimal student architectures\n",
        "    - Knowledge Distillation to transfer knowledge from teacher to student\n",
        "    - Multi-objective optimization for accuracy, speed, and model size\n",
        "    - Deployment optimization with ONNX and quantization\n",
        "\n",
        "     ðŸ“Š Key Results:\n",
        "    - 2-4x model compression with minimal accuracy loss\n",
        "    - 2-3x inference speedup on CPU\n",
        "    - Cross-platform deployment with ONNX and quantization\n",
        "\n",
        "     Upload an image to classify it with optimized models!\n",
        "    (Note: Models are trained on CIFAR-10, so they work best with images of: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, or trucks)*\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Single Model Inference\"):\n",
        "        gr.Markdown(\"### Test a single optimized model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_image = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "\n",
        "                if len(available_models) > 0:\n",
        "                    model_dropdown = gr.Dropdown(\n",
        "                        choices=list(available_models.keys()),\n",
        "                        value=list(available_models.keys())[0],\n",
        "                        label=\"Select Model\"\n",
        "                    )\n",
        "                else:\n",
        "                    model_dropdown = gr.Dropdown(\n",
        "                        choices=[\"No models available\"],\n",
        "                        value=\"No models available\",\n",
        "                        label=\"Select Model\"\n",
        "                    )\n",
        "\n",
        "                classify_button = gr.Button(\"Classify\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                prediction_output = gr.Markdown(label=\"Predictions\")\n",
        "                inference_time_output = gr.Markdown(label=\"Performance\")\n",
        "                prediction_chart = gr.Label(label=\"Confidence Scores\")\n",
        "\n",
        "        classify_button.click(\n",
        "            fn=classify_image,\n",
        "            inputs=[input_image, model_dropdown],\n",
        "            outputs=[prediction_output, inference_time_output, prediction_chart]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Model Comparison\"):\n",
        "        gr.Markdown(\" Compare all models on the same image\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                compare_input_image = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "                compare_button = gr.Button(\"Compare All Models\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                comparison_output = gr.Markdown(label=\"Comparison Results\")\n",
        "\n",
        "        compare_button.click(\n",
        "            fn=compare_models,\n",
        "            inputs=[compare_input_image],\n",
        "            outputs=[comparison_output]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"About\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "         ðŸ“– About This Project\n",
        "\n",
        "        Pipeline Overview:\n",
        "\n",
        "        1. Model Profiling: Analyze baseline models (parameters, FLOPs, latency)\n",
        "        2. Knowledge Distillation: Experiment with different distillation strategies\n",
        "        3. Search Space Design: Define architecture and hyperparameter spaces\n",
        "        4. Neural Architecture Search: Evolutionary algorithm finds optimal configs\n",
        "        5. Full Training: Train best configurations on complete dataset\n",
        "        6. Deployment Optimization: ONNX conversion and quantization\n",
        "        7. Demo Interface: This interactive showcase!\n",
        "\n",
        "        Technologies Used:\n",
        "        - PyTorch for model training\n",
        "        - ONNX for cross-platform deployment\n",
        "        - Gradio for web interface\n",
        "        - Evolutionary Algorithms for NAS\n",
        "\n",
        "        Key Features:\n",
        "        - Automated architecture search\n",
        "        - Multi-objective optimization\n",
        "        - Knowledge distillation\n",
        "        - Hardware-aware optimization\n",
        "        - Production-ready deployment formats\n",
        "\n",
        "         Results:\n",
        "        - 2-4x smaller models\n",
        "        - 2-3x faster inference\n",
        "        - <2% accuracy loss\n",
        "        - Cross-platform compatible\n",
        "\n",
        "        ---\n",
        "\n",
        "        Built with: PyTorch â€¢ ONNX â€¢ Gradio â€¢ NumPy\n",
        "\n",
        "        Project Structure: 7 Jupyter notebooks covering the complete ML pipeline\n",
        "        \"\"\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    --- General Information:\n",
        "    ðŸ’¡ Tip: For best results, upload images similar to CIFAR-10 classes (vehicles, animals, etc.)\n",
        "    \"\"\")\n",
        "\n",
        "print(\"âœ“ Gradio interface built successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DuhxvEMpxdU",
        "outputId": "9911d7cd-e042-495d-e869-b57cb0b21a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Gradio interface built successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# launch Demo\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LAUNCHING DEMO\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸš€ Starting Gradio interface...\")\n",
        "print(\"\\nðŸ“ Available models:\")\n",
        "for model_name in available_models.keys():\n",
        "    print(f\"  âœ“ {model_name}\")\n",
        "\n",
        "if len(available_models) == 0:\n",
        "    print(\"\\nâš  Warning: No models loaded. Demo will run in limited mode.\")\n",
        "    print(\"   Make sure to run Notebooks 5 & 6 first to generate models.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Demo will launch below. Share the public link to showcase your work!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Launch with share=True to get public URL\n",
        "demo.launch(\n",
        "    share=True,  # Creates public URL\n",
        "    debug=True,\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "0UQOT02vqxrY",
        "outputId": "eca110e4-d39a-4b06-bee3-013d10a5c975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LAUNCHING DEMO\n",
            "================================================================================\n",
            "\n",
            "ðŸš€ Starting Gradio interface...\n",
            "\n",
            "ðŸ“ Available models:\n",
            "\n",
            "âš  Warning: No models loaded. Demo will run in limited mode.\n",
            "   Make sure to run Notebooks 5 & 6 first to generate models.\n",
            "\n",
            "================================================================================\n",
            "Demo will launch below. Share the public link to showcase your work!\n",
            "================================================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://da167199fd80f963b7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://da167199fd80f963b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://da167199fd80f963b7.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}