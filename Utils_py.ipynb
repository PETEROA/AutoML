{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrwmbboXW883244UyfQula",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PETEROA/AutoML/blob/main/Utils_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "D27a9F3ukagc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Tuple, Optional\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Directory Configuration\n",
        "# ============================================================================\n",
        "\n",
        "def get_project_paths():\n",
        "    \"\"\"Get all project directory paths\"\"\"\n",
        "    paths = {\n",
        "        'output': Path('/mnt/user-data/outputs'),\n",
        "        'profiles': Path('/mnt/user-data/outputs/profiles'),\n",
        "        'models': Path('/mnt/user-data/outputs/models'),\n",
        "        'configs': Path('/mnt/user-data/outputs/configs'),\n",
        "        'results': Path('/mnt/user-data/outputs/results'),\n",
        "        'checkpoints': Path('/mnt/user-data/outputs/checkpoints'),\n",
        "    }\n",
        "\n",
        "\n",
        "    for path in paths.values():\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    return paths"
      ],
      "metadata": {
        "id": "95vF5WV6lANO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Model Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def count_parameters(model: nn.Module) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Count total and trainable parameters in a model\n",
        "\n",
        "    Returns:\n",
        "        (total_params, trainable_params)\n",
        "    \"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "\n",
        "def get_model_size_mb(model: nn.Module) -> float:\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
        "    size_mb = (param_size + buffer_size) / 1024**2\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "def measure_inference_time(\n",
        "    model: nn.Module,\n",
        "    input_data: torch.Tensor,\n",
        "    num_runs: int = 100,\n",
        "    warmup_runs: int = 10,\n",
        "    device: str = 'cpu'\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Measure model inference time\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        input_data: Input tensor\n",
        "        num_runs: Number of inference runs for timing\n",
        "        warmup_runs: Number of warmup runs\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with timing statistics\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    input_data = input_data.to(device)\n",
        "\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup_runs):\n",
        "            _ = model(input_data)\n",
        "\n",
        "    # Synchronize if using GPU\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Measure time\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            start = time.time()\n",
        "            _ = model(input_data)\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "            times.append(time.time() - start)\n",
        "\n",
        "    times = np.array(times) * 1000  # Convert to milliseconds\n",
        "\n",
        "    return {\n",
        "        'mean_ms': float(np.mean(times)),\n",
        "        'std_ms': float(np.std(times)),\n",
        "        'min_ms': float(np.min(times)),\n",
        "        'max_ms': float(np.max(times)),\n",
        "        'median_ms': float(np.median(times))\n",
        "    }\n"
      ],
      "metadata": {
        "id": "D2AebIVTo9Jm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Data Loading Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def load_profile_data(profile_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Load model profile data from JSON\"\"\"\n",
        "    with open(profile_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def save_profile_data(profile_data: Dict[str, Any], save_path: Path):\n",
        "    \"\"\"Save model profile data to JSON\"\"\"\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(profile_data, f, indent=2)\n",
        "\n",
        "\n",
        "def load_all_profiles() -> Dict[str, Dict]:\n",
        "    \"\"\"Load all model profiles\"\"\"\n",
        "    paths = get_project_paths()\n",
        "    profile_file = paths['output'] / 'all_model_profiles.json'\n",
        "\n",
        "    if profile_file.exists():\n",
        "        return load_profile_data(profile_file)\n",
        "    else:\n",
        "        return {'vision_models': {}, 'language_models': {}}\n"
      ],
      "metadata": {
        "id": "8gPIx6Gcpp5W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Checkpoint Management\n",
        "# ============================================================================\n",
        "\n",
        "def save_checkpoint(\n",
        "    model: nn.Module,\n",
        "    optimizer: Optional[torch.optim.Optimizer],\n",
        "    epoch: int,\n",
        "    loss: float,\n",
        "    metrics: Dict[str, float],\n",
        "    save_path: Path\n",
        "):\n",
        "    \"\"\"Save training checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'loss': loss,\n",
        "        'metrics': metrics\n",
        "    }\n",
        "\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(\n",
        "    model: nn.Module,\n",
        "    checkpoint_path: Path,\n",
        "    optimizer: Optional[torch.optim.Optimizer] = None\n",
        ") -> Tuple[nn.Module, int, float]:\n",
        "    \"\"\"\n",
        "    Load training checkpoint\n",
        "\n",
        "    Returns:\n",
        "        (model, epoch, loss)\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    return model, checkpoint['epoch'], checkpoint['loss']"
      ],
      "metadata": {
        "id": "quK8z6qmn2s4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Distillation Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def compute_kl_divergence_loss(\n",
        "    student_logits: torch.Tensor,\n",
        "    teacher_logits: torch.Tensor,\n",
        "    temperature: float = 3.0\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute KL divergence loss for knowledge distillation\n",
        "\n",
        "    Args:\n",
        "        student_logits: Student model logits\n",
        "        teacher_logits: Teacher model logits\n",
        "        temperature: Softmax temperature\n",
        "    \"\"\"\n",
        "    student_soft = torch.nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
        "    teacher_soft = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
        "\n",
        "    kl_loss = torch.nn.functional.kl_div(\n",
        "        student_soft,\n",
        "        teacher_soft,\n",
        "        reduction='batchmean'\n",
        "    ) * (temperature ** 2)\n",
        "\n",
        "    return kl_loss\n",
        "\n",
        "\n",
        "def compute_feature_matching_loss(\n",
        "    student_features: torch.Tensor,\n",
        "    teacher_features: torch.Tensor,\n",
        "    normalize: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute feature matching loss\n",
        "\n",
        "    Args:\n",
        "        student_features: Student intermediate features\n",
        "        teacher_features: Teacher intermediate features\n",
        "        normalize: Whether to normalize features\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        student_features = torch.nn.functional.normalize(student_features, dim=-1)\n",
        "        teacher_features = torch.nn.functional.normalize(teacher_features, dim=-1)\n",
        "\n",
        "    loss = torch.nn.functional.mse_loss(student_features, teacher_features)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "-XDr8Flgn8P3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Compression Ratio Calculations\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_compression_metrics(\n",
        "    teacher_params: int,\n",
        "    student_params: int,\n",
        "    teacher_size_mb: float,\n",
        "    student_size_mb: float,\n",
        "    teacher_time_ms: float,\n",
        "    student_time_ms: float\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Calculate compression metrics\"\"\"\n",
        "    return {\n",
        "        'param_compression_ratio': teacher_params / student_params,\n",
        "        'size_compression_ratio': teacher_size_mb / student_size_mb,\n",
        "        'speedup': teacher_time_ms / student_time_ms,\n",
        "        'param_reduction_pct': (1 - student_params / teacher_params) * 100,\n",
        "        'size_reduction_pct': (1 - student_size_mb / teacher_size_mb) * 100,\n",
        "        'speedup_pct': (student_time_ms / teacher_time_ms - 1) * -100\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1brKBLrLoAqQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Logging and Visualization Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def format_number(num: float, precision: int = 2) -> str:\n",
        "    \"\"\"Format large numbers with suffixes (K, M, B)\"\"\"\n",
        "    if num >= 1e9:\n",
        "        return f\"{num / 1e9:.{precision}f}B\"\n",
        "    elif num >= 1e6:\n",
        "        return f\"{num / 1e6:.{precision}f}M\"\n",
        "    elif num >= 1e3:\n",
        "        return f\"{num / 1e3:.{precision}f}K\"\n",
        "    else:\n",
        "        return f\"{num:.{precision}f}\"\n",
        "\n",
        "\n",
        "def print_model_summary(model_name: str, profile_data: Dict[str, Any]):\n",
        "    \"\"\"Pretty print model summary\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Parameters: {format_number(profile_data.get('total_params', 0))}\")\n",
        "    print(f\"  Model Size: {profile_data.get('model_size_mb', 0):.2f} MB\")\n",
        "    print(f\"  Inference Time: {profile_data.get('inference_time_ms', 0):.2f} ms\")\n",
        "    if 'throughput_fps' in profile_data:\n",
        "        print(f\"  Throughput: {profile_data['throughput_fps']:.2f} FPS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Device Management\n",
        "# ============================================================================\n",
        "\n",
        "def get_device(prefer_cuda: bool = True) -> torch.device:\n",
        "    \"\"\"Get the appropriate device for computation\"\"\"\n",
        "    if prefer_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"Using CPU\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4Xi9MfdJoYTu"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}